<!DOCTYPE html>
<html lang="en">

<head>
    <style>
        .center {
        width: 100%;
        }

        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            overflow: hidden;
        }

        .full-width-video {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        .caption {
            text-align: center;
            margin-top: auto; /* Pushes the caption to the bottom of the container */
        }        
        .transparent-image {
            opacity: 0.7; /* Set opacity to 50% */
        }

    </style>
    
    <!-- for carousel of results to work -->
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

</head>

<head>
    <!-- Title -->
    <title>Inference-Time Policy Steering</title>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Inference-Time Policy Steering with Human Interactions">
    <meta name="keywords" content="Foundation Models, Generative Policies, Inference-Time Sampling, Imitation Learning, Motion Editing">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <!-- https://fontawesome.com/cheatsheet -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-79592980-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-79592980-2');
    </script>

</head>


<body>
    <!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> -->
    <nav class="navbar navbar-expand-md fixed-top navbar-dark" style="background-color: #A31F34;">
        <a class="navbar-brand" href="#">Steering Pre-Trained Policy with Inference-Time User Input</a>

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">

            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarToggle">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link" href="#">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Abstract">Abstract</a>
                </li>
                <!-- <li class="nav-item">
                    <a class="nav-link" href="#Paper">Paper</a>
                </li>               -->
                <li class="nav-item">
                    <a class="nav-link" href="#Motivation">Motivation</a>
                <!-- <li class="nav-item">
                    <a class="nav-link" href="#Talk">Talk</a>
                </li> -->
                <li class="nav-item">
                    <a class="nav-link" href="#Method">Method</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Experiments">Experiments</a>
                </li>
                <!-- <li class="nav-item">

                    <a class="nav-link" href="#Related">Related Works</a>
                </li>                   -->
            </ul>
        </div>
    </nav>
    <br>
    <div class="container" style="padding-top: 80px; font-size: 20px">
        <div align="center">
            <h2 class="text-center" align="center">
                Inference-Time Policy Steering through Human Interactions
            </h2><br>
            <h6>
                <a href="https://yanweiw.github.io/" target="_blank">Yanwei Wang<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://liruiw.github.io/" target="_blank">Lirui Wang<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://yilundu.github.io/" target="_blank">Yilun Du<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://balakumar-s.github.io/" target="_blank">Balakumar
                    Sundaralingam<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://www.xuningyang.com/" target="_blank">Xuning
                    Yang<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=48Y9F-YAAAAJ&hl=en" target="_blank">Yu-Wei Chao<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://ai.stanford.edu/~cdarpino/" target="_blank">Claudia
                    Pérez-D’Arpino<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://interactive.mit.edu/about/people/julie" target="_blank">Julie Shah<sup>1</sup></a><br>
            </h6>
            <small><sup>1</sup>Interactive Robotics Lab / MIT CSAIL</small>
            &nbsp;&nbsp;&nbsp;&nbsp;
            <small><sup>2</sup>NVIDIA Seattle Robotics Lab</small>
        </div>
    </div><br><br>


        
    <div class="container">
        <div align="center">
            <video src="figs/itps_teaser_final_small.mp4" type="video/mp4" controls="controls" class="center">
            </video>
        </div>
    </div><br><br>


    <!-- Abstract -->
    <div class="container">
        <h4 id="Abstract" style="padding-top: 70px; margin-top: -80px; ">Abstract - What is ITPS?</h4>
        <hr>

        <div align="center" style="padding-top: 10px; font-size: 20px">
            <div class="center">
                <img class="img-responsive img-rounded" src="figs/framework.jpg" style="width:100%" alt="">
            </div>
        </div><br>

        <div style="text-align: justify">
        Generative policies trained with human demonstrations are capable of achieving multimodal and long-horizon tasks
        autonomously. However, during inference time humans are often removed from the policy execution loop, limiting the
        ability to steer pre-trained policy output towards a specific subgoal or trajectory shape among a set of multimodal
        predictions. Naive approaches to human intervention might inadvertently exacerbate covariate shifts, leading to
        downstream constraint violation or execution failures. To improve the alignment of policy output and human intent
        without causing out-of-distribution mistakes, we propose a Inference-Time Policy Steering (ITPS) framework that uses
        human interactions to bias the generative sampling process instead of fine-tuning the policy on interaction data. We
        experiment with 3 simulations and real-world benchmarks to test 3 forms of human interactions and their associated
        alignment distance metric. Out of the 6 sampling strategies we evaluate, our proposed stochastic sampling method
        combined with diffusion policy achieves the best alignment and distribution shift trade-off.
        </div>
    </div><br><br>

    <!-- Paper -->
    <!-- <div class="container">
        <h4 id="Paper" style="padding-top: 70px; margin-top: -80px;">Paper</h4>
        <hr>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2403.17124">
                    <papertitle>Grounding Language Plans in Demonstrations through Counterfactual Perturbations</papertitle>
                </a><br>
                <strong>Yanwei Wang</strong>,
                Tsun-Hsuan Wang,
                Jiayuan Mao,
                Michael Hagenow,
                Julie Shah
              <em><br>
              <a href="https://arxiv.org/abs/2403.17124">arxiv</a> /
              <a href="https://openreview.net/forum?id=qoHeuRAcSl">review</a> /
              <a href="https://github.com/yanweiw/glide">code</a> /
              <a href="https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325">MIT News</a> /
              <a href="https://techcrunch.com/2024/03/25/large-language-models-can-help-home-robots-recover-from-errors-without-human-help/">techcrunch</a>
              <br>
              <strong>ICLR 2024</strong> (<strong style="color:red;">Spotlight</strong>, acceptance rate: 5%)<br>
              </em><br>
            </td>

    </div><br><br> -->

    <div class="container">
        <h4 id="Motivation" style="padding-top: 30px; margin-top: -40px;">Motivation - What is the challenge?</h4>
        <hr>
        <div align="center">
            <video src="figs/itps_motivation.mp4" type="video/mp4" controls="controls" class="center">
            </video>
        </div>
    </div><br><br>

    <div class="container">
        <h4 id="Method" style="padding-top: 30px; margin-top: -40px;">Method - How to steer policy without adding distribution shift?</h4>
        <hr>

        <div style='text-align: justify; width: 100%; font-size: 15pt; color:#A31F34'>
            Three interaction inputs: point, sketch, physical correction
        </div>

        <div align="center">
            <video src="figs/itps_interaction.mp4" type="video/mp4" controls="controls" class="center">
            </video>
        </div><br>

        <div style='text-align: justify; width: 100%; font-size: 15pt; color:#A31F34'>
            Six sampling strategies
            <p></p>
        </div>

        <div>
            We consider 6 sampling strategies: Random Sampling (<strong>RS</strong>), Output Perturbation (<strong>OP</strong>), Post-Hoc Ranking (<strong>PR</strong>), Biased Initialization (<strong>BI</strong>), Guided Diffusion (<strong>GD</strong>), and Guided Stochastic Sampling (<strong>SS</strong>). <strong>RS, OP</strong> and <strong>PR</strong> are applied to the policy output and are agnostic to policy class. We experiment with two policy classes: Action Chunking Transformer (<strong>ACT</strong>) and Diffusion Policy (<strong>DP</strong>) in this work. <strong>BI, GD</strong> and <strong>SS</strong> are unique to duffusion and are applied to the noise input and diffusion process of the policy respectively. To implement conditional sampling based on user interactions, we use hand-crafted L2 distance metric to convert point and sketch inputs into inference-time cost objectives <strong>&#958;(&#183;)</strong> to compose with the frozen policy. Physical correction input overwrites the policy output and is only compatible with <strong>OP</strong>. We use a maze navigation task to illustrate how the six sampling strategies balance inference-time user alignment and constraint satisfaction, where <strong>alignment</strong> is measured by the L2 distance between the user input and the policy output and <strong>constraint satisfaction</strong> corresponds to maintaining collision-free.
            <p></p>
        </div>

        <div style='text-align: justify; width: 100%; font-size: 15pt; color:#A31F34'>
            Output Perturbation
            <p></p>
        </div>
        <div class="row">
            <table class="center">
                <tr>
                    <td style="width: 33%; text-align: center;">
                        <img src='figs/method_op.png' width="45%">
                        <p></p>
                        <p style="width: 90%; margin: 0 auto; text-align: justify;">User input (sketch or physical corretions) overwrites the agent (red) states in real-time (click-n-drag the mouse). <strong>OP</strong> maximizes policy alignment at the cost of potential distribution shift. Predictions in collision turn white. </p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <div style="width: 95%; margin: 0 auto;">
                            <video src="figs/act_motion_manifold.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;">Exploring the learned motion manifold of <strong>ACT</strong> (execute user inputs and visualize policy predictions)</p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <div style="width: 95%; margin: 0 auto;">
                            <video src="figs/dp_motion_manifold.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;">Exploring the learned motion manifold of <strong>DP</strong> (execute user inputs and visualize policy predictions)</p>
                    </td>
                </tr>
            </table>
        </div>

        <div style='text-align: justify; width: 100%; font-size: 15pt; color:#A31F34'>
            Post-Hoc Ranking
            <p></p>
        </div>
        <div class="row">
            <table class="center">
                <tr>
                    <td style="width: 33%; text-align: center;">
                        <img src='figs/method_pr.png' width="45%">
                        <p style="width: 90%; margin: 0 auto; text-align: justify;">
                            User input (point or sketch) is used to rank policy outputs by L2 similarity. <strong>PR</strong> introduces minimal distribution shift but only improves alignment if there already exists aligned samples in the unconditional predictions.
                    </p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <p style="width: 85%; margin: 0 auto; text-align: justify;">
                            As seen above, <strong>ACT</strong> does not produce a diverse set of predictions, leading to limited alignment improvement with <strong>PR</strong>. <strong>DP</strong>, however, exhibits higher degree of distribution multimodality and constraint satisfaction after being driven to OOD locations. 
                            Hence, <strong>PR</strong> can improve alignment, but not modify unconditional samples to be more similar to user input.
                        </p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <div style="width: 95%; margin: 0 auto;">
                            <video src="figs/maze_ph.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;"><strong>PR</strong> selects the best <strong>DP</strong> output based on sketch</p>
                        </p>
                    </td>
                </tr>
            </table>
        </div>


        <div style='text-align: justify; width: 100%; font-size: 15pt; color:#A31F34'>
            Biased Initialization
            <p></p>
        </div>
        <div class="row">
            <table class="center">
                <tr>
                    <td style="width: 33%; text-align: center;">
                        <img src='figs/method_bi.png' width="45%">
                        <p></p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <p style="width: 85%; margin: 0 auto; text-align: justify;">
                            User input (point or sketch) is used to initialize the initial noise distribution (instead of Guassian) for a <strong>DP</strong>. Similar to <strong>PR</strong>, <strong>BI</strong> offers user limited control as the diffusion sampling process is still unconditional. 
                        </p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <div style="width: 95%; margin: 0 auto;">
                            <video src="figs/maze_bi.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;"><strong>BI</strong> biases the noise distribution input of <strong>DP</strong>
                        </p>
                    </td>
                </tr>
            </table>
        </div>

        <div style='text-align: justify; width: 100%; font-size: 15pt; color:#A31F34'>
            Guided Diffusion
            <p></p>
        </div>
        <div class="row">
            <table class="center">
                <tr>
                    <td style="width: 33%; text-align: center;">
                        <img src='figs/method_gd.png' width="45%">
                        <p></p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <p style="width: 85%; margin: 0 auto; text-align: justify;">
                            User input (point or sketch) is used to guide the diffusion process with gradients of L2 similarity between the user input and the policy output. Different from <strong>PR</strong> and <strong>BI</strong>, <strong>GD</strong> can discover new trajectories close to user input that do not necessarily live on the original motion manifold (see stacking experiments). Hence, there is no guarantee that the execution will still staisfy the original constraints and be successful eventually.
                        </p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <div style="width: 95%; margin: 0 auto;">
                            <video src="figs/maze_gd.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;"><strong>GD</strong> guides sampling with gradients of L2 similarity
                        </p>
                    </td>
                </tr>
            </table>
        </div>

        <div style='text-align: justify; width: 100%; font-size: 15pt; color:#A31F34'>
            Stochastic Sampling
            <p></p>
        </div>
        <div class="row">
            <table class="center">
                <tr>
                    <td style="width: 33%; text-align: center;">
                        <img src='figs/method_rd.png' width="55%">
                        <p></p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <p style="width: 85%; margin: 0 auto; text-align: justify;">
                            <strong>SS</strong> is an improved version of <strong>GD</strong> that can generate trajectories closer to user input while maintaining the original motion constraints. Repeating each guided diffusion step M steps by adding back noise in a MCMC style effectively samples the correct gradients (direct adding denoising and alignment gradients in <strong>GD</strong> is in fact mathematically incorrect) of the composed distribution (original policy distribution composed with inference-time user objectives). 
                        </p>
                    </td>

                    <td style="width: 33%; text-align: center;">
                        <div style="width: 95%; margin: 0 auto;">
                            <video src="figs/maze_rd.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;"><strong>SS</strong> achieves the best alignment-constraint satisfaction trade-off.
                        </p>
                    </td>
                </tr>
            </table>
        </div>
    </div><br><br>


    <div class="container">
        <h4 id="Experiments" style="padding-top: 30px; margin-top: -40px;">Experiments</h4>
        <hr>
        
        <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
            Maze2D Navigation Task - Continuous Motion Alignment
            <p></p>
        </div>
        <div>
            Qualitative results of benchmarking six steering strategies with two pre-trained policies on a 2D maze navigation task. <strong>The central challenge of inference-time policy steering</strong> is to trade off between alignment with user input and staying in distribution of the pre-trained policy to maintain constraint satisfaction. We show <strong>SS</strong> is the best at producing aligned success.
            <p></p>
        </div>

        <!-- <div class="hero-body"> -->
            <div class="container" style="width: 100%; margin: 0 auto;">
                <div id="results-carousel" class="carousel results-carousel" style="width: 100%; margin: 0 auto;">
                    <div class="item item-steve">
                        <img src="figs/maze_quantitative.jpg" alt="exp02_act_op" style="width: auto;">
                    </div>
                    <div class="item item-steve">
                        <img src="figs/maze_collision_heatmap.jpg" alt="exp02_act_op" style="width: auto;">
                    </div>
                    <div class="item item-steve">
                        <img src="figs/maze_method_comp_1.jpg" alt="exp02_act_op" style="width: auto;">
                    </div>
                    <div class="item item-steve">

                        <img src="figs/maze_method_comp_2.jpg" alt="exp02_act_ph" style="width: auto;">
                    </div>
                    <div class="item item-steve">
                        <img src="figs/maze_method_comp_3.jpg" alt="exp02_dp_bi" style="width: auto;">
                    </div>
                </div>
            </div><br>
        <!-- </div> -->

        <div class="row">
            <table class="center">
                <tr>
                    <td style="width: 33%; text-align: center;">
                        <div style="width: 90%; margin: 0 auto;">
                            <video src="figs/exp02_dp_bi_evolution.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;">Denoising Evolution under <strong>BI</strong></p>
                    </td>
                    <td style="width: 33%; text-align: center;">
                        <div style="width: 90%; margin: 0 auto;">
                            <video src="figs/exp02_dp_gd_evolution.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;">Denoising Evolution under <strong>GD</strong></p>
                    </td>
                    <td style="width: 33%; text-align: center;">
                        <div style="width: 90%; margin: 0 auto;">
                            <video src="figs/exp02_dp_rd_evolution.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                        </div>
                        <p style=" text-align: center;">Denoising Evolution under <strong>SS</strong></p>
                    </td>
                </tr>
            </table>
        </div><br>

        <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
            Block Stacking Task - Discrete Ordering Alignment
            <p></p>
        </div>
        <div>
            Given a pre-trained block stacking DP policy that randomly picks a block and place it on another random block until a tower is built, we show a user can guide the policy with sketches to build a specific tower in a particular order (alignment) without exacerbating covriate shift (failed grasps or placement). We show <strong>SS</strong> strategy below where 2D sketches are projected to the Y-Z plane through the end-effector to compute the L2 similarity metric. One can also use VR controllers to draw 3D sketches to guide the policy as shown in the paper. 
            <p></p>
        </div>
        <div align="center">
            <video src="figs/block_interact.mp4" type="video/mp4" controls="controls" class="center">
            </video>
        </div><br>

        <div>
            To highlight the difference between <strong>PR</strong> and <strong>SS</strong>, we show <strong>PR</strong> cannot only improve alignment unless the policy predictions already contain user-aligned trajectories, while <strong>SS</strong> can generate novel plans with user-defined shapes even if they are missing from the initial samples. Additionally, we show one can choose whether to sample plans closer to user input or policy training distribution by addjusting the number of diffusion steps that are guided by user input.
        </div><br>
        <div class="row">
            <table class="center">
                <tr>
                    <td style="width: 33%; text-align: center;">
                        <div style="width: 90%; margin: 0 auto;">
                            <video src="figs/block_pr.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                            <p style=" text-align: center;"><strong>PR</strong> cannot generate user-aligned plans if they are missing from initial samples </p>

                        </div>
                    </td>
                    <td style="width: 33%; text-align: center;">
                        <div style="width: 90%; margin: 0 auto;">
                            <video src="figs/block_ss_all.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                            <p style=" text-align: center;"><strong>SS</strong> generates OOD plans close to user input if alignment gradients are added to all diffusion steps</p>
                        </div>
                    </td>
                    <td style="width: 33%; text-align: center;">
                        <div style="width: 90%; margin: 0 auto;">
                            <video src="figs/block_ss_early_stop.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                            <p style=" text-align: center;"><strong>SS</strong> generates plans closer to training distribution if alignment gradients are only added to early diffusion steps</p>
                        </div>
                    </td>
                </tr>
            </table>
        </div><br>

        <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
            PushT Task - Composing Multiple Inference-Time Objectives
            <p></p>
        </div>
        <div>
            To illustrate our framework's flexibility to compose multiple inference-time objectives, we show a user can use both positive sketches as preferences and negative sketches as collision constraints to shape the policy output. We take pretrained pushT policies and real-time interative version of pushT from this <a href="https://github.com/alexander-soare/little_experiments/blob/main/action_multimodality.md">LeRobot branch</a> and use gray lines to represent positive sketches that bias sampling towards them and black lines to represent negative sketchs that bias sampling away from them. Note the original pushT domain does not have constraints and the policy is trained to push the block to the goal location.
        </div><br>
        <div class="row">
            <table class="center">
                <tr>
                    <td style="width: 50%; text-align: center;">
                        <div style="width: 95%; margin: 0 auto;">
                            <video src="figs/pusht_compose_1.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                            <p style=" text-align: center;">Exploring pre-trained <strong>DP</strong> and <strong>ACT</strong> by visualizing predictions at mouse positions
                            </p>
                        </div>
                    </td>
                    <td style="width: 50%; text-align: center;">
                        <div style="width: 95%; margin: 0 auto;">
                            <video src="figs/pusht_compose_2.mp4" type="video/mp4" controls="controls" class="center">
                            </video>
                            <p style=" text-align: center;">
                                Compositing positive (gray line) and negative (black line) sketches to shape policy output
                            </p>
                        </div>
                    </td>
                </tr>
            </table>
        </div><br>

        <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
            Real-World Kitchen Task - Discrete Ordering Alignment
            <p></p>
        </div>
        <div>
            Visualizing the learned motion manifold of pre-trained <strong>DP</strong> shows multimodality only at limited parts of the state space. We show <strong>OP</strong> can improve alignment with physical corrections but suffers from covariate shift and thus grasp failures. Meanwhile, it is hard to tune guide ratio for point input with <strong>GD</strong>. While small guide ratio does not change the policy output, large guide ratio can lead to OOD incoherent plans. The best steering strategy is <strong>SS</strong> with point input that generates aligned plans close to user input while maintaining the original motion manifold. <p></p>
        </div>
        
        <div class="container" style="width: 100%; margin: 0 auto;">
            <div id="results-carousel" class="carousel results-carousel" style="width: 100%; margin: 0 auto;">
                <div class="item item-steve">
                    <video src="figs/kitchen_video_1.mp4" type="video/mp4" controls="controls" class="center">
                </div>
                <div class="item item-steve">
                    <video src="figs/kitchen_video_2.mp4" type="video/mp4" controls="controls" class="center">
                </div>
                <div class="item item-steve">
                    <video src="figs/kitchen_video_3.mp4" type="video/mp4" controls="controls" class="center">
                </div>
                <div class="item item-steve">
                    <video src="figs/kitchen_video_4.mp4" type="video/mp4" controls="controls" class="center">
                </div>
                <div class="item item-steve">
                    <video src="figs/kitchen_video_5.mp4" type="video/mp4" controls="controls" class="center">
                </div>
            </div>
        </div><br>
    </div><br>




    <!-- Poster -->
    <!-- <div class="container">
        <h4 id="Related" style="padding-top: 30px; margin-top: -40px;">Related Works - Prior work that GLiDE extends by learning instead of engineering sensor models</h4>
        <hr>


        <table class="center">
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='figs/robot_1.jpg' width="100%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://yanweiw.github.io/tli/">
                    <papertitle>Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from Demonstrations
                    </papertitle>
                </a>
                <br>
                <strong>Yanwei Wang</strong>,
                Nadia Figueroa, Shen Li, Ankit Shah, Julie Shah
                <em><br>
                    <a href="https://arxiv.org/abs/2206.04632">arxiv</a>
                    /
                    <a href="https://github.com/yanweiw/tli">code</a>
                    /
                    <a href="https://yanweiw.github.io/tli/">project page</a><br>
                    <strong>CoRL 2022</strong> (<strong style="color:red;">Oral</strong>, acceptance rate: 6.5%) <br>
                    <strong>IROS 2023 Workshop</strong> (<strong style="color:red;"> Best Student Paper</strong>, Learning Meets
                    Model-based Methods for Manipulation and Grasping Workshop)
                </em><br>
                <p>We present a continuous motion imitation method that can provably satisfy any discrete plan specified by a
                    Linear Temporal Logic (LTL) formula. Consequently, the imitator is robust to both task- and motion-level
                    disturbances and guaranteed to achieve task success.</p>
            </td>
        </table>
    </div><br><br> -->

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>

</html>